adData <- data.frame(diagnosis,predictors)
View(AlzheimerDisease)
View(adData)
str(adData)
args(createDataPartition)
library(caret)
args(createDataPartition)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
training
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
createDataPartition(mixtures$CompressiveStrength, p = 3/4)
hist(concrete$SuperPlasticizer)
str(concrete)
hist(training$SuperPlasticizer)
str(training)
unique(training$Superplasticizer)
length(training$Superplasticizer)
class(training$Superplasticizer)
mode(training$Superplasticizer)
hhist(training$Superplasticizer)
hist(training$Superplasticizer)
hist(training$Superplasticizer)
hist(training$Superplasticizer, prob = F)
hist(training$Superplasticizer, prob = T)
hist(training$Superplasticizer)
hist(log(training$Superplasticizer)
)
sum(training$Superplasticizer == 0)
log(0)
hist(training$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training)
names(training)
n <- grep("IL", names(training))
n
n <- grep("IL", names(training), fixed =T)
n
regexpr([^IL$])
regexpr(/^IL$/)
regexpr(^IL$)
n <- grep("^IL", names(training), fixed =T)
n <- grep("/^IL/", names(training), fixed =T)
n
n <- grep("/^IL/", names(training))
n
pattern <- "/^IL/"
grep(pattern, names(training))
pattern <- "^IL"
grep(pattern, names(training))
n <- grep("^IL", names(training))
n
str(training[, n])
pp <- preProcess(training[, n], thresh = 0.8)
pp
summary(pp)
args(preProcess)
pp <- preProcess(training[, n], method = "pca", thresh = 0.8)
pp <- preProcess(training[, n], method = "pca", thresh = 0.8)
predict(pp, training)
predict(pp, training[, n])
train_PCs <- predict(pp, training[, n])
dim(train_PCs)
ncol(train_PCs)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
n <- grep("^IL", names(training))
n
n <- grep("^IL", names(training))
training <- training[, n]
pp <- preProcess(training, mmethod = "pca", thresh = .8)
train_PCs <- predict(pp, training)
training = adData[ inTrain,]
names(training)
training <- training[, c(1, n)]
training
names(training)
pp <- preProcess(training, mmethod = "pca", thresh = .8) # create preprocess object
pp <- preProcess(training[, n],  method = "pca", thresh = .8) # create preprocess object
pp <- preProcess(training[, n], method = "pca", thresh = 0.8) # create preprocess object
str(training)
str(training[, n])
n
pp <- preProcess(training[, -1], method = "pca", thresh = 0.8) # create preprocess object
train_PCs <- predict(pp, training) # calc. Principal Components for train data
train_PCs <- predict(pp, training) # calc. Principal Components for train data
train_PCs <- predict(pp, training[, -1]) # calc. Principal Components for train data
model_fit_PC <- train(training$diagnosis ~., method = "glm", data = train_PCs)
test_PCs <- predict(pp, testing)
test_PCs <- predict(pp, testing[, -1])
test_PCs <- predict(pp, testing[, n])
confusionMatrix(testing$diagnosis, predict(model_fit_PC, test_PCs))
model_fir <- train(training$diagnosis ~., method = "glm", data = training)
model_fit <- train(training$diagnosis ~., method = "glm", data = training)
rm(model_fir)
confusionMatrix(testing$diagnosis, predic(model_fit, testing))
confusionMatrix(testing$diagnosis, predict(model_fit, testing))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Find all the predictor variables in the training set that begin with IL
n <- grep("^IL", names(training))
pp <- preProcess(training[, n], method = "pca", thresh = 0.9)
train_PCs <- predict(pp, training[, n])
# number of Principla components required to capture 80% of the variance
ncol(train_PCs)
#### question 4 ####
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Create a training data set consisting of only the predictors
# with variable names beginning with IL and the diagnosis.
n <- grep("^IL", names(training))
training <- training[, c(1, n)]
#  Build two predictive models, one using the predictors as they are and one ...
# using PCA with principal components explaining 80% of the variance in the predictors.
# Use method="glm" in the train function.
# What is the accuracy of each method in the test set? Which is more accurate?
### Model fir using PCA for principal components
pp <- preProcess(training[, -1], method = "pca", thresh = 0.8) # create preprocess object
train_PCs <- predict(pp, training[, -1]) # calc. Principal Components for train data
# run model on outcome
model_fit_PC <- train(training$diagnosis ~., method = "glm", data = train_PCs)
# create PCs for test set
test_PCs <- predict(pp, testing[, n])
# compare results
confusionMatrix(testing$diagnosis, predict(model_fit_PC, test_PCs))
### Using Predictors as they are
model_fit <- train(training$diagnosis ~., method = "glm", data = training)
confusionMatrix(testing$diagnosis, predict(model_fit, testing))
install.packages(c("car", "curl", "e1071", "kernlab"))
library(AppliedPredictiveModeling)
data("segmentationOriginal")
View(segmentationOriginal)
segData <- subset(segmentationOriginal, Case == "Train")
cellID <- segData$Cell
class <- segData$Class
case <- segData$Case
segData <- segData[, -(1:3)]
statusColNum <- grep("Status", names(segData))
segData <- segData[, -statusColNum]
library(e1071)
skewness(segData$AngleCh1)
skewValues <- apply(segData, skewness)
skewValues <- apply(segData, skewness())
skewValues <- apply(segData, 2, skewness)
skewValues
library(caret)
x <- rnorm(100)
skewness((x))
skewness(x)
hist(x)
Ch1AreaTrans <- BoxCoxTrans(segData$AngleCh1)
Ch1AreaTrans
Ch1AreaTrans <- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans
getwd()
setwd("~/Courses/JH_DS_Machine_Learning/project")
getwd()
library("curl", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.2")
curl ?
? curl
training <- read.csv(curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
"))
training <- read.csv(curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
"))
training <- read.csv(curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
"), open = "r")
library("RCurl", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.2")
training <- read.csv(curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
"))
t <- curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
")
is.open(t)
isOpen(t)
training <- read.cv2("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
")
training <- read.csv2("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
")
treain_url <- url(https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
treain_url <- url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
treain_url <- curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
test_url <- curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
training <- read.csv2(train_url, header = T)
train_url <- curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
training <- read.csv2(train_url, header = T)
View(training)
rm(training)
training <- read.csv2(train_url, header = T, sep = ",")
training <- read.csv2(train_url, header = T, sep = ",")
isOpen(train_url)
train_url
train_url <- curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
train_url
isOpen(train_url)
training <- read.csv2(train_url, header = T, sep = ",")
rm(segData)
rm(segmentationOriginal)
testing <- read.sv2(test_url, header = T, sep = ",")
testing <- read.csv2(test_url, header = T, sep = ",")
View(testing)
names(training)
sapply(names(training), class)
View(testing)
variables <- names(training)
variables == names(testing)
names(training[[160]])
names(testing[[160]])
names(testing[160])
names(training[160])
library(dply)
library(dplyr)
training <- tbl_df(training); testing <- tbl_df(testing)
testing
training
sum(variables == names(testing))
which (variables != names(testing))
which(variables == "classe")
testing[160]
which(names(testing) == "classe")
names(testing) == "classe"
setwd("~/Courses/JH_DS_Machine_Learning/project")
training <- read.csv2("pml-training.csv", header = T, sep = ",")
testing <- read.csv2("pml-testing.csv", header = T, sep = ",")
variables
library(dplyr)
training <- tbl_df(training); testing <- tbl_df(testing)
variables <- names(training)
variables
training$classe
table(training$classe)
library(caret)
nearZeroVar(training)
sapply(training, 2, is.na)
apply(training, 2, is.na)
sum(apply(training, 2, is.na))
x <- apply(training, 2, is.na)
dim(x)
apply(x, 2, sum)
y <- apply(x, 2, sum)
which(y != 0)
length(which(y != 0))
variables
View(training)
apply(training, 2, sum)
View(training)
sum(training[, 18])
sum(is.na(training[, 18]))
x
apply(x, 2, sum)
y <- apply(x, 2, sum)
y[18]
which(y != 0, useNames = F)
z <- which(y != 0)
class(z)
length(z)
mode(z)
is.vector((z))
is.vector(z)
training1 <- subset(training, select = -c(index))
na <- apply(trainin,2, is.na)
sumna <- apply(na, 2, sum)
index <- which(sumna != 0)
training1 <- subset(training, select = -c(index))
na <- apply(trainin,2, is.na)
na <- apply(training,2, is.na)
sumna <- apply(na, 2, sum)
index <- which(sumna != 0)
training1 <- subset(training, select = -c(index))
length(index)
160-67
names(training)
which (names(training) != names(testing))
c(c(1, 2), iindex)
c(c(1, 2), index)
training1 <- subset(training, select = -c(1:7, index))
names(training1)
index <- nearZeroVar(training1)
index
training2 <- subset(training1, select = -c(index))
index <- nearZeroVar(training1[, -length(names(training1))])
training2 <- subset(training1, select = -c(index))
correlations <- cor(training2[, -length(names(training2))])
cor(training2)
apply(training2, 2, is.numeric)
View(training2)
apply(training2, 2, is.factor)
sum(training2$roll_belt)
training <- read.csv2("pml-training.csv", header = T, sep = ",")
testing <- read.csv2("pml-testing.csv", header = T, sep = ",")
library(caret)
# Since the data set is large I am going to ommit as predictors the columns with NAs
na <- apply(training,2, is.na)
sumna <- apply(na, 2, sum)
index <- which(sumna != 0)
training1 <- subset(training, select = -c(1:7, index))
index <- nearZeroVar(training1)
training2 <- subset(training1, select = -c(index))
correlations <- cor(training2)
correlations <- cor(as.numeric(training2))
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myValidation <- trainin2[, -inTrain]
myValidation <- training2[, -inTrain]
preProcess(myTraining, model = "pca")
myTraining <- training2[, inTrain]
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myTraining <- training2[, inTrain]
myValidation <- training2[, -inTrain]
myTraining <- training2[, inTrain]
dim(inTrain)
training2[, inTrain]
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myTraining <- training2[inTrain, ]
myValidation <- training2[-inTrain, ]
preProcess(myTraining, model = "pca")
apply(myTraining, 2, as.numeric)
myTraining <- training2[inTrain, ]
myValidation <- training2[-inTrain, ]
apply(myTraining, 2, as.integer)
myTraining <- training2[inTrain, ]
View(myTraining)
as.numeric(myTraining[,1)
as.numeric(myTraining[,1])
as.numeric(myTraining[,2])
as.numeric(myTraining[,3])
as.numeric(myTraining[,4])
as.numeric(myTraining[,5])
as.numeric(myTraining[,6])
as.numeric(myTraining[,53])
preProcess(myTraining[, -53], method = "pca")
training <- read.csv2("pml-training.csv", header = T, sep = ",", stringsAsFactors = F)
testing <- read.csv2("pml-testing.csv", header = T, sep = ",", stringsAsFactors = F)
na <- apply(training,2, is.na)
sumna <- apply(na, 2, sum)
index <- which(sumna != 0)
# I am also ommiting the irrelevat columns that keep logisitc info for the subjects {columns 1:7}
training1 <- subset(training, select = -c(1:7, index))
# next I filter for near-zero variance predictors, since a tree based model (classification) ...
# is impervious to this type of predictor since it would never be used in a split.
index <- nearZeroVar(training1)
training2 <- subset(training1, select = -c(index))
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myTraining <- training2[inTrain, ]
myValidation <- training2[-inTrain, ]
preProcess(myTraining[, -53], method = "pca")
training <- read.csv2("pml-training.csv", header = T, sep = ",", na.strings = ".")
testing <- read.csv2("pml-testing.csv", header = T, sep = ",", na.strings = ".")
na <- apply(training,2, is.na)
sumna <- apply(na, 2, sum)
index <- which(sumna != 0)
# I am also ommiting the irrelevat columns that keep logisitc info for the subjects {columns 1:7}
training1 <- subset(training, select = -c(1:7, index))
# next I filter for near-zero variance predictors, since a tree based model (classification) ...
# is impervious to this type of predictor since it would never be used in a split.
index <- nearZeroVar(training1)
training2 <- subset(training1, select = -c(index))
# split the training set into myTraining 60% and myValidation 40% sets
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myTraining <- training2[inTrain, ]
myValidation <- training2[-inTrain, ]
preProcess(myTraining[, -53], method = "pca")
training <- read.csv2("pml-training.csv", header = T, sep = ",", na.strings = c("NA", "", "#DIV/0!"))
na <- apply(training,2, is.na)
sumna <- apply(na, 2, sum)
index <- which(sumna != 0)
# I am also ommiting the irrelevat columns that keep logisitc info for the subjects {columns 1:7}
training1 <- subset(training, select = -c(1:7, index))
# next I filter for near-zero variance predictors, since a tree based model (classification) ...
# is impervious to this type of predictor since it would never be used in a split.
index <- nearZeroVar(training1)
training2 <- subset(training1, select = -c(index))
# split the training set into myTraining 60% and myValidation 40% sets
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myTraining <- training2[inTrain, ]
myValidation <- training2[-inTrain, ]
na <- apply(training,2, is.na)
sumna <- apply(na, 2, sum)
index <- which(sumna != 0)
training1 <- subset(training, select = -c(1:7, index))
# next I filter for near-zero variance predictors, since a tree based model (classification) ...
# is impervious to this type of predictor since it would never be used in a split.
index <- nearZeroVar(training1)
training2 <- subset(training1, select = -c(index))
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
index <- nearZeroVar(training1)
training <- read.csv2("pml-training.csv", header = T, sep = ",", na.strings = c("", "#DIV/0!"))
na <- apply(training,2, is.na)
sumna <- apply(na, 2, sum)
index <- which(sumna != 0)
training1 <- subset(training, select = -c(1:7, index))
index <- nearZeroVar(training1)
training2 <- subset(training1, select = -c(index))
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myTraining <- training2[inTrain, ]
myValidation <- training2[-inTrain, ]
preProcess(myTraining, method = "pca")
# Set the working directory
setwd("~/Courses/JH_DS_Machine_Learning/project")
#*****************************************************************************************
# Read the data directly from internet
library(curl)
train_url <- curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
test_url <- curl("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
training <- read.csv2(train_url, header = T, sep = ",")    # training data set
testing <- read.csv2(test_url, header = T, sep = ",")    # testing data set
variables <- names(training)
#*****************************************************************************************
# or read the data from the hard drive to make it quicker
training <- read.csv2("pml-training.csv", header = T, sep = ",")
testing <- read.csv2("pml-testing.csv", header = T, sep = ",")
library(dplyr)
training <- tbl_df(training); testing <- tbl_df(testing)
# there is a dirrenece in the column name of column 160
# sum(variables == names(testing))
which (names(training) != names(testing)) # see below
# The goal of your project is to predict the manner in which they did the exercise.
# This is the "classe" variable in the training set.
# You may use any of the other variables to predict with.
# You should create a report describing how you built your model,
#       how you used cross validation,
#       what you think the expected out of sample error is,
#       and why you made the choices you did.
# You will also use your prediction model to predict 20 different test cases.
library(caret)
# Since the data set is large I am going to ommit as predictors the columns with NAs
na <- apply(training,2, is.na)
sumna <- apply(na, 2, sum)
index <- which(sumna != 0)
# I am also ommiting the irrelevat columns that keep logisitc info for the subjects {columns 1:7}
training1 <- subset(training, select = -c(1:7, index))
# next I filter for near-zero variance predictors, since a tree based model (classification) ...
# is impervious to this type of predictor since it would never be used in a split.
index <- nearZeroVar(training1)
training2 <- subset(training1, select = -c(index))
# split the training set into myTraining 60% and myValidation 40% sets
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myTraining <- training2[inTrain, ]
myValidation <- training2[-inTrain, ]
View(myTraining)
apply(myTraining[, -53], 2, as.numeric)
myTraining <- apply(myTraining[, -53], 2, as.numeric)
preProcess(myTraining, method ="pca")
training2 <- apply(training2[, -53], 2, as.numeric)
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myTraining <- training2[inTrain, ]
myValidation <- training2[-inTrain, ]
training2 <- subset(training1, select = -c(index))
training2 <- subset(training1, select = -c(index))
myTraining <- apply(myTraining[, -53], 2, as.numeric)
training2 <- lapply(training2[, -53], 2, as.numeric)
training2 <- as.data.frame(apply(training2[, -53], 2, as.numeric))
training2 <- subset(training1, select = -c(index))
training2 <- tbl_df(apply(training2[, -53], 2, as.numeric))
training2 <- as.data.frame(apply(training2[, -53], 2, as.numeric))
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myTraining <- training2[inTrain, ]
myValidation <- training2[-inTrain, ]
training2 <- subset(training1, select = -c(index))
training2 <- as.data.frame(apply(training2, 2, as.numeric))
training2 <- subset(training1, select = -c(index))
training2 <- cbind(as.data.frame(apply(training2[, -53], 2, as.numeric)), training2[, 53])
inTrain <- createDataPartition(y = training2$classe, p = 0.6, list = F)
myTraining <- training2[inTrain, ]
myValidation <- training2[-inTrain, ]
preProc <- preProcess(myTraining[, -53], method = "pca")
